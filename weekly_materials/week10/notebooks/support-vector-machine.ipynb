{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)\n",
    "\n",
    "Support vector machine is another popular machine learning algortihm for both supervised classification and regression type problems. The objective of the machine leaning algorithms we learnt in the past week was to minimize loss function. The loss functions were closely related to prediction errors. In support vector machine, the objective function we want to maximize is called `margin`. The concept of `margin` is much intuitive to understand in a binary classfication problem setting as below.\n",
    "\n",
    "![](../files/points.png)\n",
    "\n",
    "Consider a classification problem as below where the objective is to find a function that can well separate the brown and blue sample of blobs. It is easy to see that a simple line or a linear function can act like a discriminant function or decision boundary between two groups. But as show in the below figure, there could be many many lines that can accurately separate the two clusters. How do we choose which line is the best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../files/line1.png)\n",
    "![](../files/line2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from last week that the goal of a predictive model is to develop a model that generalizes well with the future data. Hence among many possible lines, we should opt for a line that works out equally well with future data set. SVM focuses on the closest samples in two clusters and tries to pick a line that has the greatest distance from those samples. The intuition is that the samples that are more likely to get misclassified in the future data set will be in the nearby region in the feature space as those closets ones. As such,the line with the greatest distance from the candidate points that are most difficult to classify in the training data will also likely have bettter chance to generalize than any other possible lines.\n",
    "\n",
    "Support vectors are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane. Margin is a separation of line to the closest points.  Support vectors help us maximize the margin of the classifier. Take a look at the two dotted lines in the below figures. When the margin the dotted line is narrower than below in the close proximity to the solid line, the number of support vectors as shown as circled samples will be decreased and the line choosen as such might overfit the training data. By widening the dotted lines and choosing more samples as support vectors might slightly decrease the training accuracy but with better generalization capacity.\n",
    "\n",
    "![](../files/c001.png) \n",
    "![](../files/c01.png)\n",
    "![](../files/c1.png)\n",
    "![](../files/c10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving nonlinear problem \n",
    "\n",
    "We saw how SVM can handle linearly separable data. Unfortunately, most of the real world datsets are not linearly seperable. SVM tackles a non-linear problem by using a method called kernel method to project dataset into a trasformed linearly separable feature space.\n",
    "\n",
    "In the below figure, the data is not linearly separable in the feature space defined by x1 and x2. The basic idea behind the kernel methods is to create a nonlinear combinations of the original features to project them onto a higher dimensional space via a mapping function (phi) where it becomes linearly separable.\n",
    "\n",
    "$$\\phi(x_1,x_2)= (z_1, z_2, z_3) = (x_1, x_2, x_1^2  + x_2^2)$$ \n",
    "\n",
    "Then we can train a linear SVM model to classify the data into the new trasformed feature space. The following dataset in two dimensional feature space can be transformed into a three dimensional one where the the classes become separable. When the data is projected back into original feature space, the boundary between the classes will not be a hyperplance so as margins and support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../files/nonlinear.png)\n",
    "\n",
    "[Source: Python Machine Learning Chapter 3](https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch03/ch03.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is hard to know in general what projections would transform the feature space into a linearaly separable one, and also the construction of all possible feature transformation can be computationally expensive. SVM uses so-called kernel tricks. Wikipedia defines the kernel trick as follows:\n",
    "\n",
    "```Kernel methods owe their name to the use of kernel functions, which enable them to operate in a high-dimensional, implicit feature space without ever computing the coordinates of the data in that space, but rather by simply computing the inner products between the images of all pairs of data in the feature space. This operation is often computationally cheaper than the explicit computation of the coordinates. This approach is called the \"kernel trick\"```\n",
    "\n",
    "\n",
    "The Radial basis function kernel, which can generate infite-dimensional projection, is a popular kernel function commonly used in support vector machine classification. The RBF kernel is given by:\n",
    "\n",
    "\\begin{equation*}\n",
    "k ( x^i, x^j) = exp\\Bigl(\\frac{|x^i - x^j|^2}{2\\sigma^2}\\Bigr)\n",
    "\\end{equation*},\n",
    "\n",
    "which can be simplified to, \n",
    "\n",
    "\\begin{equation*}\n",
    "k ( x^i, x^j) = exp\\Bigl(-\\gamma|x^i - x^j|^2\\Bigr)\n",
    "\\end{equation*}\n",
    "\n",
    "Here gamma is a hyperparameter, which ranges from 0 to 1. A higher value of gamma tends to cause over-fitting. Gamma=0.1 is considered to be a good default value. Determination of the best optimal gamma value for a given dataset requires experimentation and need to be tuned during model building process.\n",
    "\n",
    "\n",
    "[Here](http://pages.cs.wisc.edu/~matthewb/pages/notes/pdf/svms/RBFKernel.pdf) is the mathmatical proof if you want to know how it is capable of doing infinite-dimensional projection.\n",
    "\n",
    "\n",
    "The below figure shows the decision boundary for discriminating three Iris speof SVM classification created by \n",
    "The code from [here](https://scikit-learn.org/stable/auto_examples/svm/plot_iris_svc.html) was slightly modified to obtain the below figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../files/class4.png )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
